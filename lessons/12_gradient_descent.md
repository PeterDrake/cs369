Attendance  
Ripped From Today's Headlines

# Gradient Descent
* Idea
  * We want to find the best parameter settings for a model
    * So the space we're searching in is ***not*** the data space!
    * In the first few diagrams in the book, horizontal axis is a parameter, vertical is cost
  * We want to move to the bottom of the deepest valley
  * Mountain-in-fog analogy
    * At each step, move downhill
* WC: What is a gradient?
  * It tells you both the direction and steepness of downhill
* WC: What is a local minimum and why is it a problem?
* WC: Explain how Figure 4-7 represents 3-dimensional space

* Play with [Gradient Descent Visualization](https://github.com/lilipads/gradient_descent_viz)
  * Turn off everything but gradient descent
  * Play with the learning rate on the global minimum surface
    * What happens if it's too high?
    * What happens if it's too low?
  * Turn on gradient arrows
  * Turn on path
  * Play with the different surfaces
    * Where does this approach run into problems?
  * Advanced option
    * Turn on momentum and momentum arrows
    * Tweak parameters to get the ball into the global minimum in the plateau surface
    * Speculate on what momentum does

* WC: Explain difference between batch, stochastic, and mini-batch gradient descent
* Unpack equations 4-5 to 4-7

# For Next Time
Linear Regression is due tonight  
Book 2 is due Thursday
Read GÃ©ron, pp. 149-173
